{"cells":[{"cell_type":"code","execution_count":1,"id":"3adaf9f5","metadata":{"id":"3adaf9f5","executionInfo":{"status":"ok","timestamp":1765439497850,"user_tz":-120,"elapsed":17911,"user":{"displayName":"Lex","userId":"00842875266722598886"}}},"outputs":[],"source":["!pip install torch --quiet\n","\n","!pip install transformers sentence-transformers --quiet\n","!pip install requests tqdm --quiet"]},{"cell_type":"code","execution_count":2,"id":"c41011b7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c41011b7","executionInfo":{"status":"ok","timestamp":1765439505674,"user_tz":-120,"elapsed":6883,"user":{"displayName":"Lex","userId":"00842875266722598886"}},"outputId":"bab01b6d-d2ed-4aea-c030-4bf5b6cedef1"},"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA available: True\n","Device: Tesla T4\n"]}],"source":["import torch\n","print('CUDA available:', torch.cuda.is_available())\n","print('Device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No CUDA')\n"]},{"cell_type":"code","execution_count":3,"id":"7d284607","metadata":{"id":"7d284607","executionInfo":{"status":"ok","timestamp":1765439518221,"user_tz":-120,"elapsed":10829,"user":{"displayName":"Lex","userId":"00842875266722598886"}}},"outputs":[],"source":["#==================================================\n","#Text encoder with attention-based pooling utilities.\n","#==================================================\n","\n","import contextlib\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoModel, AutoTokenizer\n","\n","device = None\n","\n","if torch.backends.mps.is_available():\n","\tdevice = torch.device('mps')\n","elif torch.cuda.is_available():\n","\tdevice = torch.device('cuda')\n","else:\n","\tdevice = torch.device('cpu')\n","\n","\n","epsilon = 1e-8\n","\n","class Norm:\n","\t# Project to unit sphere\n","\t#--------------------------------------------------\n","\t@staticmethod\n","\tdef to_sphere(v):\n","\t\tresult = v\n","\t\tif v.dim() == 1:\n","\t\t\tresult = v / (v.norm() + epsilon)\n","\t\telse:\n","\t\t\tresult = v / (v.norm(dim=1, keepdim=True) + epsilon)\n","\t\treturn result\n","\n","\n","\t# Alias for to_sphere\n","\t#--------------------------------------------------\n","\t@staticmethod\n","\tdef s(v):\n","\t\tresult = Norm.to_sphere(v)\n","\t\treturn result\n","\n","\n","\t# Project to hypercube\n","\t#--------------------------------------------------\n","\t@staticmethod\n","\tdef to_hypercube(v):\n","\t\tif v.dim() == 1:\n","\t\t\tm = v.abs().max()\n","\t\t\treturn v / (m + epsilon)\n","\t\tm = v.abs().max(dim=1, keepdim=True).values\n","\t\treturn v / (m + epsilon)\n","\n","\n","\t# Alias for to_hypercube\n","\t#--------------------------------------------------\n","\t@staticmethod\n","\tdef h(v):\n","\t\tresult = Norm.to_hypercube(v)\n","\t\treturn result\n","\n","\n","class Encoder:\n","\tdef __init__(self, model_name=None):\n","\t\tdefault_model_name = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n","\t\tself.model_name    = model_name or default_model_name\n","\t\tself.tokenizer     = AutoTokenizer.from_pretrained(self.model_name)\n","\t\tself.model         = AutoModel.from_pretrained(self.model_name, attn_implementation='eager').to(device)\n","\t\tself.model.eval()\n","\n","\n","\t# Attention pooling over hidden states\n","\t# --------------------------------------------------\n","\tdef attention_pool(self, hidden_states, attentions):\n","\t\t'''\n","\t\thidden_states: [S, D]\n","\t\tattentions: list of [batch, heads, S, S] for each layer\n","\t\t'''\n","\t\tatt = torch.stack(attentions)   # [L, B, H, S, S]\n","\t\tatt = att[:, 0]                 # take batch 0 → [L, H, S, S]\n","\t\tatt = att.mean(dim=1)           # average over heads → [L, S, S]\n","\t\tatt = att.mean(dim=0)           # average over layers → [S, S]\n","\n","\t\tweights = att[0]\n","\t\tweights = F.softmax(weights, dim=0)\n","\n","\t\tap_next = (weights.unsqueeze(1) * hidden_states).sum(dim=0)\n","\t\tresult = Norm.to_hypercube(ap_next)\n","\t\treturn result\n","\n","\n","\t# Encode a single text\n","\t# --------------------------------------------------\n","\tdef encode(self, text, ap_prev=None, karma=1):\n","\t\tamp_ctx = torch.cuda.amp.autocast if device.type == 'cuda' else contextlib.nullcontext\n","\t\twith torch.inference_mode(), amp_ctx():\n","\t\t\ttokens = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=1000)\n","\t\t\ttokens = {k: v.to(device) for k, v in tokens.items()}\n","\n","\t\t\tout    = self.model(**tokens, output_attentions=True)\n","\t\t\thidden = out.last_hidden_state.squeeze(0)\n","\t\t\tattn   = out.attentions\n","\n","\t\t\tap_next = self.attention_pool(hidden, attn)\n","\n","\t\t\tif ap_prev is not None:\n","\t\t\t\tap_prev = ap_prev.to(device)\n","\t\t\t\tap_next = ap_next + karma * ap_prev\n","\t\t\t\tap_next = Norm.to_hypercube(ap_next)\n","\n","\t\treturn ap_next.to('cpu')\n","\n","\t# Encode a sequence of texts\n","\t# --------------------------------------------------\n","\tdef encode_sequence(self, texts, ap_prev=None, karma=1):\n","\t\tembeddings = []\n","\t\tfor text in texts:\n","\t\t\tap_next = self.encode(text, ap_prev=ap_prev, karma=karma)\n","\t\t\tembeddings.append(ap_next)\n","\t\tresult = torch.stack(embeddings)\n","\t\treturn result\n","\n","\tdef test_encode_batch(self, texts, sample_size=20, batch_size=32):\n","\t\timport random\n","\t\timport torch.nn.functional as F\n","\n","\t\t# choose sample\n","\t\tif len(texts) > sample_size:\n","\t\t\tsample = random.sample(texts, sample_size)\n","\t\telse:\n","\t\t\tsample = texts\n","\n","\t\tprint('\\n=== encode() vs encode_batch_magic() consistency test ===')\n","\t\tprint(f'Comparing {len(sample)} texts...\\n')\n","\n","\t\t# 1) magic batch encode (no padding → identical)\n","\t\tap_batch = self.encode_batch(sample)\n","\n","\t\tsims = []\n","\n","\t\t# 2) compare one-by-one\n","\t\tfor i, t in enumerate(sample):\n","\t\t\tap_single = self.encode(t)\n","\t\t\tap_b = ap_batch[i]\n","\n","\t\t\tsim = F.cosine_similarity(\n","\t\t\t\tap_single.unsqueeze(0),\n","\t\t\t\tap_b.unsqueeze(0)\n","\t\t\t).item()\n","\n","\t\t\tsims.append(sim)\n","\t\t\tprint(f'[{i:02d}] sim={sim:.6f} | text={t}')\n","\n","\t\tsims = torch.tensor(sims)\n","\n","\t\tprint('\\n=== Summary ===')\n","\t\tprint(f'Min similarity:  {sims.min().item():.6f}')\n","\t\tprint(f'Max similarity:  {sims.max().item():.6f}')\n","\t\tprint(f'Mean similarity: {sims.mean().item():.6f}')\n","\n","\t\treturn sims\n","\n","\t@torch.inference_mode()\n","\tdef encode_batch(self, texts, ap_prev_batch=None, karma=1, batch_size=32):\n","\t\tamp_ctx = torch.cuda.amp.autocast if device.type == 'cuda' else contextlib.nullcontext\n","\t\twith amp_ctx():\n","\t\t\t# Tokenize\n","\t\t\ttokens = self.tokenizer(\n","\t\t\t\ttexts,\n","\t\t\t\treturn_tensors='pt',\n","\t\t\t\tpadding=True,\n","\t\t\t\ttruncation=True\n","\t\t\t)\n","\t\t\ttokens = {k: v.to(device) for k, v in tokens.items()}\n","\n","\t\t\tout = self.model(**tokens, output_attentions=True)\n","\n","\t\t\thidden = out.last_hidden_state      # [B, S, D]\n","\t\t\tattns  = out.attentions             # tuple(L) of [B, H, S, S]\n","\t\t\tmask   = tokens['attention_mask']   # [B, S]\n","\n","\t\t\tap_list = []\n","\t\t\tB = hidden.size(0)\n","\n","\t\t\tfor b in range(B):\n","\t\t\t\tseq_len = mask[b].sum().item()   # ignore padding so outputs match encode()\n","\t\t\t\thidden_b = hidden[b, :seq_len]   # [S, D]\n","\n","\t\t\t\t# Build tuple of [H, S, S] tensors as encode() expects\n","\t\t\t\t# Unsqueeze batch dim to match attention_pool()'s [B, H, S, S] expectation\n","\t\t\t\tatt_b = tuple(layer[b, :, :seq_len, :seq_len].unsqueeze(0) for layer in attns)  # tuple(L), each [1, H, S, S]\n","\n","\t\t\t\tap_next = self.attention_pool(hidden_b, att_b)\n","\n","\t\t\t\t# Optional recurrence\n","\t\t\t\tif ap_prev_batch is not None:\n","\t\t\t\t\tap_prev = ap_prev_batch[b].to(device)\n","\t\t\t\t\tap_next = ap_next + karma * ap_prev\n","\t\t\t\t\tap_next = Norm.to_hypercube(ap_next)\n","\n","\t\t\t\tap_list.append(ap_next.cpu())\n","\n","\t\treturn torch.stack(ap_list)\n","\n"]},{"cell_type":"code","source":["#==================================================\n","# Load 20 samples and test batch encoding\n","#==================================================\n","\n","import random\n","\n","# 1. Load 20 random sentences from dataset\n","path = '/content/en_ru.sentences.1M.txt'\n","\n","with open(path, 'r', encoding='utf-8') as f:\n","\tlines = [line.strip() for line in f if line.strip()]\n","\n","sample = random.sample(lines, 20)\n","\n","print(f'Loaded {len(sample)} samples:')\n","for i, s in enumerate(sample):\n","\tprint(f'[{i:02d}] {s[:80]!r}')\n","\n","# 2. Initialize encoder\n","enc = Encoder()\n","\n","# 3. Run batch consistency test\n","enc.test_encode_batch(sample, sample_size=20, batch_size=20)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TTAyryX7sIdv","executionInfo":{"status":"ok","timestamp":1765439544139,"user_tz":-120,"elapsed":22937,"user":{"displayName":"Lex","userId":"00842875266722598886"}},"outputId":"4d3fa08d-5203-4575-cc79-83dbffa70794"},"id":"TTAyryX7sIdv","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 20 samples:\n","[00] 'I was just kind of like your stand-in.'\n","[01] 'И если они ещё продлятся, запасы двигателей на Западе закончатся к тому моменту,'\n","[02] \"There's no place like home.\"\n","[03] 'Господи боже. обещает 60 человек.'\n","[04] 'Почему мне кажется, что это нужно только мне?'\n","[05] 'Confirm, assault team of two to the beach.'\n","[06] 'И поставил их жизнь под угрозу.'\n","[07] 'Да, на всякий случай, чтобы произвести хорошее впечатление, чтобы они подумали..'\n","[08] 'Он он сказал, что может подождать пока тебе не станет лучше.'\n","[09] 'Каре будет интересно где мы находимся.'\n","[10] 'Jeremiah got the top score on eight Advanced Placement tests without ever taking'\n","[11] 'Я не могу... Оно настоящее.'\n","[12] 'Может быть, нужно просто смириться и жить с этим и делать то, что нужно делать.'\n","[13] 'Не говоря ни слова, они могут угрожать, умолять, манить, велеть...'\n","[14] 'Почему вы солгали нам о покупке бара у Джаффа Китсона, хотя продал его вам Эл Дж'\n","[15] 'No, no, no, no, no!'\n","[16] 'Она защищала тебя от разаков.'\n","[17] '- В эту сторону. - Хорошо.'\n","[18] 'Это было несколько часов назад, до того, как все сюда пришли.'\n","[19] 'Честно, я нализался столько этой гадости сегодня, что наверняка отравился.'\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","=== encode() vs encode_batch_magic() consistency test ===\n","Comparing 20 texts...\n","\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-673748455.py:165: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with amp_ctx():\n","/tmp/ipython-input-673748455.py:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.inference_mode(), amp_ctx():\n"]},{"output_type":"stream","name":"stdout","text":["[00] sim=1.000000 | text=I was just kind of like your stand-in.\n","[01] sim=1.000000 | text=И если они ещё продлятся, запасы двигателей на Западе закончатся к тому моменту, как ваши двигатели попадут на рынок.\n","[02] sim=1.000000 | text=There's no place like home.\n","[03] sim=1.000000 | text=Господи боже. обещает 60 человек.\n","[04] sim=1.000000 | text=Почему мне кажется, что это нужно только мне?\n","[05] sim=1.000000 | text=Confirm, assault team of two to the beach.\n","[06] sim=1.000000 | text=И поставил их жизнь под угрозу.\n","[07] sim=1.000000 | text=Да, на всякий случай, чтобы произвести хорошее впечатление, чтобы они подумали...\n","[08] sim=1.000000 | text=Он он сказал, что может подождать пока тебе не станет лучше.\n","[09] sim=1.000000 | text=Каре будет интересно где мы находимся.\n","[10] sim=1.000000 | text=Jeremiah got the top score on eight Advanced Placement tests without ever taking the Advanced Placement courses.\n","[11] sim=1.000000 | text=Я не могу... Оно настоящее.\n","[12] sim=1.000000 | text=Может быть, нужно просто смириться и жить с этим и делать то, что нужно делать.\n","[13] sim=1.000000 | text=Не говоря ни слова, они могут угрожать, умолять, манить, велеть...\n","[14] sim=1.000000 | text=Почему вы солгали нам о покупке бара у Джаффа Китсона, хотя продал его вам Эл Дженкинс?\n","[15] sim=1.000000 | text=No, no, no, no, no!\n","[16] sim=1.000000 | text=Она защищала тебя от разаков.\n","[17] sim=1.000000 | text=- В эту сторону. - Хорошо.\n","[18] sim=1.000000 | text=Это было несколько часов назад, до того, как все сюда пришли.\n","[19] sim=1.000000 | text=Честно, я нализался столько этой гадости сегодня, что наверняка отравился.\n","\n","=== Summary ===\n","Min similarity:  1.000000\n","Max similarity:  1.000000\n","Mean similarity: 1.000000\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n","        1.0000, 1.0000])"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from math import ceil\n","from tqdm import tqdm\n","import torch\n","from transformers import GPT2TokenizerFast\n","\n","encoder = Encoder()\n","batch_size = 512  # adjust if memory permits\n","\n","# GPT-2 tokenizer setup\n","tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n","if tokenizer.pad_token is None:\n","\ttokenizer.pad_token = tokenizer.eos_token\n","\n","texts = []\n","ap_vectors = []\n","input_ids = []\n","attention_masks = []\n","\n","# --------------------------------------------------\n","# 1. Load all texts\n","# --------------------------------------------------\n","with open('/content/en_ru.sentences.1M.txt', 'r', encoding='utf-8') as f:\n","\tfor line in tqdm(f, desc='loading'):\n","\t\tt = line.strip()\n","\t\tif t:\n","\t\t\ttexts.append(t)\n","\n","# --------------------------------------------------\n","# 2. Batched AP encoding + GPT-2 tokenization\n","# --------------------------------------------------\n","num_batches = ceil(len(texts) / batch_size)\n","\n","for i in tqdm(range(num_batches), desc='encoding AP + tokenizing'):\n","\tbatch = texts[i * batch_size : (i + 1) * batch_size]\n","\n","\t# ---- AP encoding ----\n","\tap_batch = encoder.encode_batch(batch)  # CPU tensors\n","\tap_vectors.extend(ap_batch)\n","\n","\t# ---- GPT-2 tokenization ----\n","\ttok = tokenizer(\n","\t\tbatch,\n","\t\treturn_tensors='pt',\n","\t\tpadding='max_length',\n","\t\ttruncation=True,\n","\t\tmax_length=64\n","\t)\n","\n","\tinput_ids.append(tok['input_ids'])\n","\tattention_masks.append(tok['attention_mask'])\n","\n","# --------------------------------------------------\n","# 3. Stack everything\n","# --------------------------------------------------\n","aps    = torch.stack(ap_vectors)          # [N, 384]\n","ids    = torch.cat(input_ids, dim=0)      # [N, 40]\n","masks  = torch.cat(attention_masks, dim=0)  # [N, 40]\n","\n","# --------------------------------------------------\n","# 4. Save dataset\n","# --------------------------------------------------\n","torch.save(\n","\t{\n","\t\t'aps': aps,\n","\t\t'input_ids': ids,\n","\t\t'attention_mask': masks\n","\t},\n","\t'dataset_ap.pt'\n",")\n","\n","print('Saved dataset_ap.pt')\n"],"metadata":{"id":"Z-ew5HAink9c"},"id":"Z-ew5HAink9c","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":6,"id":"27a475e4","metadata":{"id":"27a475e4","executionInfo":{"status":"ok","timestamp":1765439558592,"user_tz":-120,"elapsed":51,"user":{"displayName":"Lex","userId":"00842875266722598886"}}},"outputs":[],"source":["#==================================================\n","# SentenceDecoder — AP → Text Generator (v0.3)\n","# Freezes GPT-2 embeddings + lower transformer blocks,\n","# trains upper blocks + LM head + AP projection layer.\n","#==================================================\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n","from transformers import get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","\n","\n","class SentenceDataset(Dataset):\n","\tdef __init__(self, path):\n","\t\tstate = torch.load(path, map_location='cpu')\n","\n","\t\tself.aps   = state['aps']              # list[tensor]       shape [384]\n","\t\tself.ids   = state['input_ids']        # list[tensor]       shape [max_len]\n","\t\tself.masks = state['attention_mask']   # list[tensor]       shape [max_len]\n","\n","\tdef __len__(self):\n","\t\treturn len(self.aps)\n","\n","\tdef __getitem__(self, idx):\n","\t\treturn (\n","\t\t\tself.aps[idx],     # AP vector\n","\t\t\tself.ids[idx],     # token ids\n","\t\t\tself.masks[idx]    # mask\n","\t\t)\n","\n","\n","\n","\n","#==================================================\n","# SentenceDecoder\n","#==================================================\n","\n","class SentenceDecoder(nn.Module):\n","\tdef __init__(self, ap_dim=384, model_name='gpt2', freeze_lower_k=10):\n","\t\tsuper().__init__()\n","\n","\t\tself.gpt       = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n","\t\tself.tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n","\n","\t\t# GPT-2 has no pad token → set pad = eos\n","\t\tif self.tokenizer.pad_token is None:\n","\t\t\tself.tokenizer.pad_token = self.tokenizer.eos_token\n","\t\t\tself.gpt.config.pad_token_id = self.tokenizer.eos_token_id\n","\n","\n","\t\thidden = self.gpt.config.hidden_size\n","\t\tself.proj = nn.Linear(ap_dim, hidden).to(device)\n","\n","\n","\t\t#==================================================\n","\t\t# FREEZE STRATEGY\n","\t\t#==================================================\n","\n","\t\t# Freeze embeddings: token + positional\n","\t\tself.gpt.transformer.wte.requires_grad_(False)\n","\t\tself.gpt.transformer.wpe.requires_grad_(False)\n","\n","\t\t# Freeze lower K decoder blocks\n","\t\t# Example: GPT-2 small has 12 layers → freeze 10 → train last 2\n","\t\tfor i, block in enumerate(self.gpt.transformer.h):\n","\t\t\tif i < freeze_lower_k:\n","\t\t\t\tfor p in block.parameters():\n","\t\t\t\t\tp.requires_grad = False\n","\n","\t\t# Keep upper blocks trainable (they remain unfrozen)\n","\t\t# Train LM head\n","\n","\t\tfor p in self.gpt.lm_head.parameters():\n","\t\t\tp.requires_grad = True\n","\n","\t\t# Projection layer always trainable\n","\t\tfor p in self.proj.parameters():\n","\t\t\tp.requires_grad = True\n","\n","\t\tself.state = {}\n","\n","\t#==================================================\n","\t# LOAD / SAVE\n","\t#==================================================\n","\n","\tdef load(self, path):\n","\t\ttry:\n","\t\t\tstate = torch.load(path, map_location=device)\n","\t\t\tself.load_state_dict(state['model'])\n","\t\t\tself.state = state.get('meta', {})\n","\t\t\treturn True\n","\t\texcept:\n","\t\t\treturn False\n","\n","\tdef save(self, path):\n","\t\ttorch.save(\n","\t\t\t{\n","\t\t\t\t'model': self.state_dict(),\n","\t\t\t\t'meta':  self.state\n","\t\t\t},\n","\t\t\tpath\n","\t\t)\n","\t\treturn path\n","\n","\t#==================================================\n","\t# INFERENCE\n","\t#==================================================\n","\n","\tdef decode(self, ap_vector, max_len=40, temperature=0.9, top_p=0.95):\n","\t\tap = Norm.to_hypercube(ap_vector).to(device)\n","\t\tprefix = self.proj(ap).unsqueeze(0).unsqueeze(1)\n","\n","\t\toutput = self.gpt.generate(\n","\t\t\tinputs_embeds   = prefix,\n","\t\t\tmax_length      = max_len,\n","\t\t\tdo_sample       = True,\n","\t\t\ttemperature     = temperature,\n","\t\t\ttop_p           = top_p,\n","\t\t\tpad_token_id    = self.tokenizer.eos_token_id\n","\t\t)\n","\n","\t\treturn self.tokenizer.decode(output[0], skip_special_tokens=True)\n","\n","\t#==================================================\n","\t# TRAINING LOOP\n","\t#==================================================\n","\n","\tdef train_model(self, path, batch_size=16, lr=3e-5, epochs=1, accum_steps=2):\n","\t\tdataset = SentenceDataset(path)\n","\t\tloader  = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","\t\tparams = filter(lambda p: p.requires_grad, self.parameters())\n","\t\toptim  = torch.optim.AdamW(params, lr=lr)\n","\n","\t\ttotal_steps = len(loader) * epochs\n","\t\tscheduler = get_linear_schedule_with_warmup(\n","\t\t\toptim,\n","\t\t\tnum_warmup_steps = int(0.05 * total_steps),\n","\t\t\tnum_training_steps = total_steps\n","\t\t)\n","\n","\t\tamp_ctx = torch.cuda.amp.autocast if device.type == 'cuda' else contextlib.nullcontext\n","\t\tscaler  = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n","\n","\t\tself.train()\n","\t\tstep_counter = 0\n","\n","\t\tfor epoch in range(epochs):\n","\t\t\tprogress = tqdm(loader, desc=f'Epoch {epoch+1}')\n","\n","\t\t\tfor ap, ids, mask in progress:\n","\t\t\t\tap   = ap.to(device)\n","\t\t\t\tids  = ids.to(device)\n","\t\t\t\tmask = mask.to(device)\n","\n","\t\t\t\tprefix = self.proj(ap).unsqueeze(1)\n","\t\t\t\tinputs_embeds = self.gpt.transformer.wte(ids)\n","\t\t\t\tinputs_embeds[:, 0, :] = prefix[:, 0, :]\n","\n","\t\t\t\twith amp_ctx():\n","\t\t\t\t\toutputs = self.gpt(\n","\t\t\t\t\t\tinputs_embeds = inputs_embeds,\n","\t\t\t\t\t\tattention_mask = mask,\n","\t\t\t\t\t\tlabels = ids\n","\t\t\t\t\t)\n","\t\t\t\t\tloss = outputs.loss / accum_steps\n","\n","\t\t\t\tif scaler:\n","\t\t\t\t\tscaler.scale(loss).backward()\n","\t\t\t\telse:\n","\t\t\t\t\tloss.backward()\n","\n","\t\t\t\tif (step_counter + 1) % accum_steps == 0:\n","\t\t\t\t\tif scaler:\n","\t\t\t\t\t\tscaler.step(optim)\n","\t\t\t\t\t\tscaler.update()\n","\t\t\t\t\telse:\n","\t\t\t\t\t\toptim.step()\n","\n","\t\t\t\t\toptim.zero_grad()\n","\t\t\t\t\tscheduler.step()\n","\n","\t\t\t\tstep_counter += 1\n","\t\t\t\tprogress.set_postfix({'loss': f'{loss.item()*accum_steps:.4f}'})\n","\n","\t\t\t# --------------------------------------------------\n","\t\t\t# SAVE AFTER EACH EPOCH\n","\t\t\t# --------------------------------------------------\n","\t\t\tsave_path = f'sentence_decoder_epoch{epoch+1}.pt'\n","\t\t\ttorch.save(self.state_dict(), save_path)\n","\t\t\tprint(f'Epoch {epoch+1} saved → {save_path}')\n","\n"]},{"cell_type":"code","source":["print(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iE2yb_CclFjX","executionInfo":{"status":"ok","timestamp":1765439564354,"user_tz":-120,"elapsed":5,"user":{"displayName":"Lex","userId":"00842875266722598886"}},"outputId":"b429b5ef-f573-4440-a215-6bcd73874acd"},"id":"iE2yb_CclFjX","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","execution_count":null,"id":"0f756f2b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0f756f2b","outputId":"774b5635-3742-42d6-b9fd-0ad51079e5e1"},"outputs":[{"output_type":"stream","name":"stdout","text":["SentenceDecoder initialized.\n","Starting training...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3549239964.py:145: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler  = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n","Epoch 1:   0%|          | 0/125000 [00:00<?, ?it/s]/tmp/ipython-input-3549239964.py:162: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with amp_ctx():\n","`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n","Epoch 1:   0%|          | 452/125000 [00:38<2:25:22, 14.28it/s, loss=6.3348]"]}],"source":["#==================================================\n","# CONFIG\n","#==================================================\n","\n","dataset_path = '/content/dataset_ap.pt'\n","\n","\n","#--------------------------------------------------\n","# 2. Load SentenceDecoder\n","#--------------------------------------------------\n","model = SentenceDecoder(\n","\tap_dim = 384,\n","\tmodel_name = 'gpt2',\n","\tfreeze_lower_k = 10\n",")\n","\n","print('SentenceDecoder initialized.')\n","\n","\n","#--------------------------------------------------\n","# 3. Train\n","#--------------------------------------------------\n","print('Starting training...')\n","\n","model.train_model(\n","\tpath       = dataset_path,   # ← dataset_ap.pt\n","\tbatch_size = 8,\n","\tlr         = 3e-5,\n","\tepochs     = 3\n",")\n","\n","print('Training complete.')\n","\n","\n","#--------------------------------------------------\n","# 4. Save\n","#--------------------------------------------------\n","save_path = 'sentence_decoder.pt'\n","torch.save(model.state_dict(), save_path)\n","\n","print(f'Model saved to {save_path}')\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}